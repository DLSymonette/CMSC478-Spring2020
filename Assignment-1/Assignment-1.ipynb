{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC478 Machine Learning - Spring 2020\n",
    "\n",
    "## Instructor: Fereydoon Vafaei\n",
    "\n",
    "### <font color=\"blue\">Assignment-1: Binary Classification, Linear Regression and Logistic Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your name and ID here* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you've learned about the basics and fundamental concepts in Machine Learning such as supervised learning (e.g. classification and regression) vs unsupervised learning (e.g. clustering), overfitting, and model evaluation. You learned that two of the most common approaches in supervised learning are regression and classification. In this assignment, you are going to pratice what you've learned so far by building ML models and applying them on data.\n",
    "\n",
    "<b>Very Important Note:</b> Read ALL the instructions in this notebook very carefully. Careless reading and skipping lines would be a major source of making mistakes and losing points in your first assignment! Also notice that this assignment has <b>three parts</b> and requires a lot of work. You're strongly recommended to get started early and plan to finish well before the due. Technical problems or other issues/questions on the due date or just a day before would NOT be accepted as an excuse to delay your submission. Also notice that some outputs are provided to you for your reference and some are not provided intentionally. You may NOT share the outputs and/or the code, the solution and the answers in Piazza.\n",
    "\n",
    "Pedagogically, this assignment will help you:\n",
    "- better understand the concepts you learned and how to use ML models in pratice. \n",
    "- practice your Python skills - and possibly learn a couple of new \"Pythonic\" tricks!\n",
    "- pratice reading documentation. This is a very important skill in AI/ML/Data Science collaborative environments and teams.\n",
    "\n",
    "So, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification means you are classifying only two classes/labels. You are going to build a binary classifier that can classify breats cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is downloading [the breast cancer dataset](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/data). Read the feature specifications in Kaggle page to learn more about the data. You can directly download the data from [the course GitHub repo here](https://github.com/fereydoonvafaei/CMSC478-Spring2020/blob/master/Assignment-1/breast_cancer.csv) and save it in the same working directory as your notebook. Next, you should load the data using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset using pd\n",
    "data = pd.read_csv('breast_cancer.csv')\n",
    "\n",
    "# Show the first five rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's see what the type of `data` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame` is a data structure to contain datasets in pandas. Read pandas documentation to learn more about it [here](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html) and [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). Since this is a classification (supervised learning) ML model, you have both the features and the labels for training and testing, and everything has been stored in `data` DataFrame. You should always check if your dataset has `NA` or `Null` values and decide about it, usually drop `NA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "diagnosis                  0\n",
       "radius_mean                0\n",
       "texture_mean               0\n",
       "perimeter_mean             0\n",
       "area_mean                  0\n",
       "smoothness_mean            0\n",
       "compactness_mean           0\n",
       "concavity_mean             0\n",
       "concave points_mean        0\n",
       "symmetry_mean              0\n",
       "fractal_dimension_mean     0\n",
       "radius_se                  0\n",
       "texture_se                 0\n",
       "perimeter_se               0\n",
       "area_se                    0\n",
       "smoothness_se              0\n",
       "compactness_se             0\n",
       "concavity_se               0\n",
       "concave points_se          0\n",
       "symmetry_se                0\n",
       "fractal_dimension_se       0\n",
       "radius_worst               0\n",
       "texture_worst              0\n",
       "perimeter_worst            0\n",
       "area_worst                 0\n",
       "smoothness_worst           0\n",
       "compactness_worst          0\n",
       "concavity_worst            0\n",
       "concave points_worst       0\n",
       "symmetry_worst             0\n",
       "fractal_dimension_worst    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Also, the first column `id` doesn't seem to provide any useful info to ML model, so drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, you can extract features and labels from `data`. Your classifier should attempt to predict `diagnosis` so that is your target/label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data to feature vector X and label vector y\n",
    "X = data.drop(['diagnosis'], axis=1)\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape:  (569, 30)\n",
      "Labels shape:  (569,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features shape: \", X.shape)\n",
    "print(\"Labels shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your `X` dataframe now only contains features, hence has 30 columns whereas `y` has now become a 1D vector containing labels only. Notice that `y` has 569 labels equal to the number of data records in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X should no longer contain the diagnosis which is target/label column - i.e. the column to be predicted\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    M\n",
       "1    M\n",
       "2    M\n",
       "3    M\n",
       "4    M\n",
       "Name: diagnosis, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y should only contain diagnosis - target/label column\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'B'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The two classes (aka labels) here are `M` and `B` representing `malignant` and `benign` which refers to the tumors you are going to classify. You need to represent them by `1` and `0` respectively. In other words, to use sklearn classifiers and score metrics, you need to convert the categorical lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical labels M and B to 1 and 0\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When you have multiple features with different scales/ranges, you should consider standardizing them. There are different ways to standardize and to normalize the feature vector. One way is using scikit-learn modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lectures, the data for supervised learning (both classification and regression) is split into training set and test set. We do this by importing another module from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can split the data. The split ratio we are going to choose is 0.75 for training and 0.25 for testing, but we only need to specify one of them `test_size` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (426, 30)\n",
      "y_train shape:  (426,)\n",
      "X_test shape:  (143, 30)\n",
      "y_test shape:  (143,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of X_train, X_test, y_train, y_test\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can now choose which classifier from the built-in classifiers in sklearn we want to use. We are going to use two classifiers that we've seen in the lectures so far: Stochastic Gradient Descent Classifier and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the following cell. You may want to consult with the textbook code snippets. Notice that we use [`.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit) method for training and [`.predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.predict) method for making predictions (testing). Click on the methods and read their documentation.\n",
    "\n",
    "Enter your code in the blocks that start with `### START CODE HERE ###`. In some cells, you simply replace <font color=\"green\"><b>None</b></font> with your code whereas in some others you enter multiple lines of your code in one cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "\n",
    "# Create a SGDClassifier with random_state=0\n",
    "sgd_clf = None\n",
    "\n",
    "# Fit sgd_clf model on training set\n",
    "None\n",
    "\n",
    "# Create a LogisticRegression classifier with solver='liblinear' and random_state=0\n",
    "log_reg_clf = None\n",
    "\n",
    "# Fit log_reg_clf on training set\n",
    "None\n",
    "\n",
    "### END CODING HERE ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, make predictions on testing set and store the results\n",
    "\n",
    "### START CODING HERE ###\n",
    "y_pred_sgd = None\n",
    "y_pred_log_reg = None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_pred_sgd), type(y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred_sgd.shape == y_pred_log_reg.shape, \"Prediction shapes don't match!\"\n",
    "print(y_pred_sgd.shape)\n",
    "print(y_pred_log_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, evaluate how your classifiers perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "\n",
    "# Import the necessary module from sklearn for accuracy_score, precision_score and recall_score \n",
    "None\n",
    "None\n",
    "None\n",
    "\n",
    "# compute the accuracy, precision and recall\n",
    "acc_sgd = None\n",
    "precision_sgd = None\n",
    "recall_sgd = None\n",
    "\n",
    "# Print the accuracy \n",
    "print(\"SGD Accuracy: \", acc_sgd)\n",
    "print(\"SGD Precision: \", precision_sgd)\n",
    "print(\"SGD Recall: \", recall_sgd)\n",
    "\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, the following cells show how you can plot ROC curve of logistic regression classifier using `fpr`, `tpr`, and `threshold` returned by [`metrics.roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and class probabilities using [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) method of [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class probabilities, fpr, tpr and threshold for Logistic Regression.\n",
    "import sklearn.metrics as metrics\n",
    "log_reg_probs = log_reg_clf.predict_proba(X_test)\n",
    "log_reg_preds = log_reg_probs[:,1]\n",
    "log_reg_fpr, log_reg_tpr, log_reg_threshold = metrics.roc_curve(y_test, log_reg_preds)\n",
    "log_reg_roc_auc = metrics.auc(log_reg_fpr, log_reg_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve for logistic regression classifier\n",
    "plt.title('ROC')\n",
    "plt.plot(log_reg_fpr, log_reg_tpr, 'b', label = 'AUC_Log_Reg = %0.2f' % log_reg_roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you should plot ROC curves of both classifiers in one plot. Notice that some classifiers like SGD don't have `predict_proba` method, and in that case you can use [`decision_function`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function) method to get the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC for both classifiers with different colors in one plot,\n",
    "# and with AUC scores shown on the 'lower right' of the plot.\n",
    "\n",
    "# Hint1: You may use LogisticRegression ROC curve as a reference for writing your code.\n",
    "# Hint2: sgd with 'hinge' loss doesn't have predict_proba method,\n",
    "# instead, you should directly use decision_function method to compute sgd_preds\n",
    "\n",
    "### START CODING HERE ###\n",
    "\n",
    "# Write as many number of lines of code as needed here.\n",
    "\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I Questions\n",
    "\n",
    "Answer the following question HERE:\n",
    "\n",
    "Q1[1 point] - What is the data type of `y_pred_sgd` and `y_pred_log_reg` and what are their shapes? Explain why you get such a shape for predictions.\n",
    "\n",
    "Q2[2 points]- Name two hyperparameters for SGD and two hyperparameters for LogisticRegression. You may check their sklearn documentation.\n",
    "\n",
    "Q3[2 points]- Looking at ROC curves of Logistic Regression and SGD, and without seeing AUC scores, how would you determine which classifier has a better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Linear Regression Using Closed-Form Solution (Normal Equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part II, you're going to generate some data and then use closed-form solution for linear regression to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data points based on the following linear equation added by [the noise with “standard normal” distribution](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html). You may refer to the slides/textbook code snippets.\n",
    "\n",
    "$$y = 3 + 5X + Gaussian Noise$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "\n",
    "### START CODING HERE ###\n",
    "# Compute y based on the linear equation given above and add random gaussian noise\n",
    "y = None\n",
    "### END CODING HERE ###\n",
    "\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "# add x0 = 1 to each instance using np.c_\n",
    "X_b = None\n",
    "\n",
    "# Compute theta_best using normal equation\n",
    "theta_best = None\n",
    "### END CODING HERE ###\n",
    "\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "X_new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "# Make predictions on new data by computing the dot product of X_new_b and theta_best\n",
    "y_predict = None\n",
    "### END CODING HERE ###\n",
    "\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III - Regression Using Least Mean Square and RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part III, you will implement three functions to build and use regression models: `train`, `use`, and `rmse` and then you apply them to some weather data.\n",
    "\n",
    "Here are the specifications for these functions, which you must satisfy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model = train(X, T, learning_rate, n_epochs, verbose)`\n",
    "* `X`: is an $N$ x $D$ matrix of input data samples, one per row. $N$ is the number of samples and $D$ is the number of variable values in\n",
    "each sample.\n",
    "* `T`: is an $N$ x $K$ matrix of desired target values for each sample.  $K$ is the number of output values you want to predict for each sample.\n",
    "* `learning_rate`: is a scalar that controls the step size of each update to the weight values.\n",
    "* `n_epochs`: is the number of epochs, or passes, through all $N$ samples, to take while updating the weight values.\n",
    "* `verbose`: is True or False (default value) to control whether or not occasional text is printed to show the training progress.\n",
    "* `model`: is the returned value, which must be a dictionary with the keys `'w'`, `'Xmeans'`, `'Xstds'`, `'Tmeans'` and `'Tstds'`.\n",
    "\n",
    "`Y = use(X, model)`\n",
    "* `X`: is an $N$ x $D$ matrix of input data samples, one per row, for which you want to predict the target values.\n",
    "* `model`: is the dictionary returned by `train`.\n",
    "* `Y`: is the returned $N$ x $K$ matrix of predicted values, one for each sample in `X`.\n",
    "\n",
    "`result = rmse(Y, T)`\n",
    "* `Y`: is an $N$ x $K$ matrix of predictions produced by `use`.\n",
    "* `T`: is the $N$ x $K$ matrix of target values.\n",
    "* `result`: is a scalar calculated as the square root of the mean of the squared differences between each sample (row) in `Y` and `T`.\n",
    "\n",
    "<b>Hints:</b>\n",
    "- Remember from regression performance measure that RMSE equation is:\n",
    "\n",
    "$\n",
    "\\text{RMSE}(\\mathbf{X}, h) = \\sqrt{\\frac{1}{m}\\sum\\limits_{i=1}^{m}\\left(h(\\mathbf{x}^{(i)}) - y^{(i)}\\right)^2}\n",
    "$\n",
    "\n",
    "where $h(\\mathbf{x}^{(i)})$ is your prediction versus $y^{(i)}$ target values. Notice that you will implement rmse twice, once in `train` function, and once as a separate function named `rmse`. In the `rmse` function when you work with vectorized form of Y and T, you can simply subtract and square them but you need to get the mean and the square root as well.<br>\n",
    "\n",
    "- Also notice that in Python, you may use `@` as matrix multiplication operator between vectors. That is a simpler notation came in recent versions of Python 3.5+ as an alternative for [`numpy.matmul`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html). This would be used between two matrices whereas when you multiply a single scalar to a matrix such as `learning_rate` multiplied by a matrix, you may use `*` operator.\n",
    "\n",
    "\n",
    "- During the training and in each iteration, weights would be updated as the following: \n",
    "`w = w + learning_rate * X[n:n + 1, :].T * error`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "def train(X, T, learning_rate, n_epochs, verbose=False):\n",
    "\n",
    "    # Calculate means and standard deviations of each column in X and T\n",
    "    Xmeans = None\n",
    "    Xstds = None\n",
    "    Tmeans = None\n",
    "    Tstds = None\n",
    "    \n",
    "    # Use the means and standard deviations to standardize X and T\n",
    "    X = None\n",
    "    T = None\n",
    "\n",
    "    # Insert the column of constant 1's as a new initial column in X\n",
    "    X = None\n",
    "    \n",
    "    # Initialize weights to be a numpy array of the correct shape and all zero values.\n",
    "    n_samples, n_inputs = None\n",
    "    n_outputs = None\n",
    "    w = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sqerror_sum = 0\n",
    "\n",
    "        for n in range(n_samples):\n",
    "\n",
    "            # Use current weight values to predict output for sample n (provided here), then\n",
    "            # calculate the error, and\n",
    "            # update the weight values.\n",
    "            y = X[n:n + 1, :] @ w      # predicted value, y, for sample n\n",
    "            error = None\n",
    "            w += None\n",
    "            \n",
    "            # Add the squared error to sqerror_sum\n",
    "            sqerror_sum += None\n",
    "            \n",
    "        if verbose and (n_epochs < 11 or (epoch + 1) % (n_epochs // 10) == 0):\n",
    "            # Compute RMSE\n",
    "            rmse = None\n",
    "            rmse = rmse[0, 0]  # because rmse is 1x1 matrix\n",
    "            print(f'Epoch {epoch + 1} RMSE {rmse:.2f}')\n",
    "\n",
    "    return {'w': w, 'Xmeans': Xmeans, 'Xstds': Xstds,\n",
    "            'Tmeans': Tmeans, 'Tstds': Tstds}\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "def use(X, model):\n",
    "    # Standardize X using Xmeans and Xstds in model\n",
    "    X = None\n",
    "    # Insert the column of constant 1's as a new initial column in X\n",
    "    X = None\n",
    "    # Predict output values using weights in model\n",
    "    Y = None\n",
    "    # Unstandardize the predicted output values using Tmeans and Tstds in model\n",
    "    Y = None\n",
    "    # Return the unstandardized output values\n",
    "    None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "def rmse(Y, T):\n",
    "    None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(0, 100).reshape(-1, 1)  # make X a 100 x 1 matrix\n",
    "T = 0.5 + 0.3 * X + 0.005 * (X - 50) ** 2\n",
    "plt.plot(X, T, '.')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('T');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your output might slightly differ due to randomness\n",
    "model = train(X, T, 0.01, 50, verbose=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = use(X, model)\n",
    "plt.plot(T, '.', label='T')\n",
    "plt.plot(Y, '.', label='Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rmse(Y, T)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y[:, 0], T[:, 0], 'o')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "a = max(min(Y[:, 0]), min(T[:, 0]))\n",
    "b = min(max(Y[:, 0]), max(T[:, 0]))\n",
    "plt.plot([a, b], [a, b], 'r', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for testing if your implementation is correct\n",
    "# You should get the provided correct output with RMSE decreasing to 0.12\n",
    "X = np.array([1, 2, 3, 4, 5, 8, 9, 11]).reshape((-1, 1))\n",
    "T = (X - 5) * 0.05 + 0.002 * (X - 8)**2\n",
    "model = train(X, T, 0.001, 1000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Output <br>\n",
    "`Epoch 100 RMSE 0.46\n",
    "Epoch 200 RMSE 0.24\n",
    "Epoch 300 RMSE 0.15\n",
    "Epoch 400 RMSE 0.13\n",
    "Epoch 500 RMSE 0.13\n",
    "Epoch 600 RMSE 0.12\n",
    "Epoch 700 RMSE 0.12\n",
    "Epoch 800 RMSE 0.12\n",
    "Epoch 900 RMSE 0.12\n",
    "Epoch 1000 RMSE 0.12`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your functions are working, we can apply them to some real data. You will use data\n",
    "from  [Colorado State University's CoAgMet Station Daily Data Access](http://coagmet.colostate.edu/cgi-bin/dailydata_form.pl).\n",
    "\n",
    "You can get the data file directly from the course GitHub repo [here](https://github.com/fereydoonvafaei/CMSC478-Spring2020/blob/master/Assignment-1/weather.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data into variable `df` using `pandas.read_csv` like you did above for binary classification.\n",
    "Notice that missing values in this dataset are indicated by the string `'***'`. This should be specified using `na_values` argument of `read_csv`. You must check if there are missing values using DataFrame `.isna().sum()` and drop them using DataFrame `.dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "df = None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "# Check if there are na's in df\n",
    "None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "# Drop na\n",
    "df = None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "# Now, check again if there is any more na values left\n",
    "None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check df columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a linear model that predicts the next day's average temperature `tave` from the previous day's values of:\n",
    "1. tave: average temperature\n",
    "2. tmax: maximum temperature\n",
    "3. tmin: minimum temperature\n",
    "4. vp: vapor pressure\n",
    "5. rhmax: maximum relative humidity\n",
    "6. rhmin: minimum relative humidity\n",
    "7. pp: precipitation\n",
    "8. gust: wind gust speed\n",
    "\n",
    "As a hint on how to do this, here is a list with these column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of column names\n",
    "Xnames = ['tave', 'tmax', 'tmin', 'vp', 'rhmax', 'rhmin', 'pp', 'gust']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "# Select those eight columns in Xnames from df and convert the result to a numpy array - Hint: use .values\n",
    "data = None\n",
    "\n",
    "# Assign X to be all columns and all but the last row - Hint: Review slicing notebook shared with you\n",
    "X = None\n",
    "\n",
    "# Assign T to be just the first column (tave) and all but the first sample/row.\n",
    "T = None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So now the first row (sample) in `X` is associated with the first row (sample) in `T` which is tave for the following day. Next, train the model. Run it several times with different `learning_rate` and `n_epochs` values to produce decreasing errors, or at least with an RMSE of less than 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "\n",
    "# Use the function train to train a model for the X and T data.\n",
    "# Try different values for learning_rate and n_epochs\n",
    "model = None\n",
    "\n",
    "# Use the use function\n",
    "None\n",
    "\n",
    "plt.clf()\n",
    "# Plot T versus predicted Y values to show how well the model is working\n",
    "None\n",
    "None\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T[100:120], '-o')\n",
    "plt.plot(Y[100:120], '-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Print the weight values in the resulting model along with their corresponding variable names in `Xnames`. Use the relative magnitude of the weight values to discuss which input variables are most significant in predicting the changes in the tave values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Xnames)):\n",
    "    print(f\"{Xnames[i]:6} {model['w'][i,0]:9.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anser the following question HERE:\n",
    "\n",
    "(5 points) Whcih features (input variables) are most significant in predicting the changes in the tave values? Name top three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Assignment-1 has a maximum of 100 points. Make sure that you get the correct outputs for all cells that you implement and give complete answers to all questions. Also, your notebook should be written with no grammatical and spelling errors and should be easy-to-read.\n",
    "\n",
    "The breakdown of the 100 points is as follows:\n",
    "\n",
    "- Part I implementaion and correct outputs have 30 points:\n",
    "    - 20 points: classifiers, training, testing and model evaluation.\n",
    "    - 10 points: correct plot of 2 ROC for 2 classifiers in one plot with different colors.\n",
    "\n",
    "- Part I questions have 5 points.\n",
    "\n",
    "- Part II normal equation (and correct outputs) has 5 points.\n",
    "\n",
    "- Part III has 60 points (55 points implementation and 5 points question).\n",
    "\n",
    "<b>Note: </b>Follow the instructions of each section carefully. Up to 10 points may be deducted if your submitted notebook is not easy to read and follow or if it has grammatical, spelling or formatting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your notebook ```Lastname-A1.ipynb```. So, for me it would be `Vafaei-A1.ipynb`. Submit the completed notebook using the ```Assignment-1``` link on Blackboard.\n",
    "\n",
    "Grading will be based on \n",
    "\n",
    "  * correct implementation, correct answer to the questions, and\n",
    "  * readability of the notebook.\n",
    "  \n",
    "<font color=red><b>Due Date: Wednesday March 4th, 11:59PM.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "\n",
    "Special thanks to Professor Charles Anderson from Colorado State University."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
